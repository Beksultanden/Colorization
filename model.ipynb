{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e50f3981-a9ee-4c42-b069-44263725d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#новый модель\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import functools\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "def get_norm_layer(norm_type='instance'):\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n",
    "    elif norm_type == 'none':\n",
    "        norm_layer = None\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
    "    return norm_layer\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, opt):\n",
    "    if opt.lr_policy == 'lambda':\n",
    "        def lambda_rule(epoch):\n",
    "            lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n",
    "            return lr_l\n",
    "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "    elif opt.lr_policy == 'step':\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n",
    "    elif opt.lr_policy == 'plateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
    "    else:\n",
    "        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "def init_weights(net, init_type='xavier', gain=0.02):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            init.normal_(m.weight.data, 1.0, gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)\n",
    "\n",
    "\n",
    "def init_net(net, init_type='xavier', gpu_ids=[]):\n",
    "    if len(gpu_ids) > 0:\n",
    "        assert(torch.cuda.is_available())\n",
    "        net.to(gpu_ids[0])\n",
    "        net = torch.nn.DataParallel(net, gpu_ids)\n",
    "    init_weights(net, init_type)\n",
    "    return net\n",
    "\n",
    "\n",
    "def define_G(input_nc, output_nc, ngf, which_model_netG, norm='batch', use_dropout=False, init_type='xavier', gpu_ids=[], use_tanh=True, classification=True):\n",
    "    netG = None\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "\n",
    "    if which_model_netG =='siggraph':\n",
    "        netG = SIGGRAPHGenerator(input_nc, output_nc, norm_layer=norm_layer, use_tanh=use_tanh, classification=classification)\n",
    "    elif which_model_netG =='instance':\n",
    "        netG = InstanceGenerator(input_nc, output_nc, norm_layer=norm_layer, use_tanh=use_tanh, classification=classification)\n",
    "    elif which_model_netG == 'fusion':\n",
    "        netG = FusionGenerator(input_nc, output_nc, norm_layer=norm_layer, use_tanh=use_tanh, classification=classification)\n",
    "    else:\n",
    "        raise NotImplementedError('Generator model name [%s] is not recognized' % which_model_netG)\n",
    "    return init_net(netG, init_type, gpu_ids)\n",
    "\n",
    "\n",
    "class HuberLoss(nn.Module):\n",
    "    def __init__(self, delta=.01):\n",
    "        super(HuberLoss, self).__init__()\n",
    "        self.delta=delta\n",
    "\n",
    "    def __call__(self, in0, in1):\n",
    "        mask = torch.zeros_like(in0)\n",
    "        mann = torch.abs(in0-in1)\n",
    "        eucl = .5 * (mann**2)\n",
    "        mask[...] = mann < self.delta\n",
    "\n",
    "        # loss = eucl*mask + self.delta*(mann-.5*self.delta)*(1-mask)\n",
    "        loss = eucl*mask/self.delta + (mann-.5*self.delta)*(1-mask)\n",
    "        return torch.sum(loss,dim=1,keepdim=True)\n",
    "\n",
    "\n",
    "class L1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L1Loss, self).__init__()\n",
    "\n",
    "    def __call__(self, in0, in1):\n",
    "        return torch.sum(torch.abs(in0-in1),dim=1,keepdim=True)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.sigmoid_channel = nn.Sigmoid()\n",
    "\n",
    "        self.conv_spatial = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel Attention\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        channel = self.sigmoid_channel(avg_out + max_out)\n",
    "        x = x * channel\n",
    "\n",
    "        # Spatial Attention\n",
    "        avg = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_val, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        spatial = torch.cat([avg, max_val], dim=1)\n",
    "        spatial = self.sigmoid_spatial(self.conv_spatial(spatial))\n",
    "        x = x * spatial\n",
    "        return x\n",
    "\n",
    "class SIGGRAPHGenerator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, norm_layer=nn.BatchNorm2d, use_tanh=True, classification=True):\n",
    "        super(SIGGRAPHGenerator, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.classification = classification\n",
    "        use_bias = True\n",
    "\n",
    "        # Conv1\n",
    "        # model1=[nn.ReflectionPad2d(1),]\n",
    "        model1=[nn.Conv2d(input_nc, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model1+=[norm_layer(64),]\n",
    "        model1+=[nn.ReLU(True),]\n",
    "        # model1+=[nn.ReflectionPad2d(1),]\n",
    "        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model1+=[nn.ReLU(True),]\n",
    "        model1+=[norm_layer(64),]\n",
    "        # add a subsampling operation\n",
    "\n",
    "        # Conv2\n",
    "        # model2=[nn.ReflectionPad2d(1),]\n",
    "        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model2+=[norm_layer(128),]\n",
    "        model2+=[nn.ReLU(True),]\n",
    "        # model2+=[nn.ReflectionPad2d(1),]\n",
    "        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model2+=[nn.ReLU(True),]\n",
    "        model2+=[norm_layer(128),]\n",
    "        # add a subsampling layer operation\n",
    "\n",
    "        # Conv3\n",
    "        # model3=[nn.ReflectionPad2d(1),]\n",
    "        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model3+=[norm_layer(256),]\n",
    "        model3+=[nn.ReLU(True),]\n",
    "        # model3+=[nn.ReflectionPad2d(1),]\n",
    "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model3+=[norm_layer(256),]\n",
    "        model3+=[nn.ReLU(True),]\n",
    "        # model3+=[nn.ReflectionPad2d(1),]\n",
    "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model3+=[nn.ReLU(True),]\n",
    "        model3+=[norm_layer(256),]\n",
    "        # add a subsampling layer operation\n",
    "\n",
    "        # Conv4\n",
    "        # model47=[nn.ReflectionPad2d(1),]\n",
    "        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model4+=[norm_layer(512),]\n",
    "        model4+=[nn.ReLU(True),]\n",
    "        # model4+=[nn.ReflectionPad2d(1),]\n",
    "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model4+=[norm_layer(512),]\n",
    "        model4+=[nn.ReLU(True),]\n",
    "        # model4+=[nn.ReflectionPad2d(1),]\n",
    "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model4+=[nn.ReLU(True),]\n",
    "        model4+=[norm_layer(512),]\n",
    "\n",
    "        # Conv5\n",
    "        # model47+=[nn.ReflectionPad2d(2),]\n",
    "        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        # model5+=[norm_layer(512),]\n",
    "        model5+=[nn.ReLU(True),]\n",
    "        # model5+=[nn.ReflectionPad2d(2),]\n",
    "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        # model5+=[norm_layer(512),]\n",
    "        model5+=[nn.ReLU(True),]\n",
    "        # model5+=[nn.ReflectionPad2d(2),]\n",
    "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        model5+=[nn.ReLU(True),]\n",
    "        model5+=[norm_layer(512),]\n",
    "        \n",
    "\n",
    "\n",
    "        # Conv6\n",
    "        # model6+=[nn.ReflectionPad2d(2),]\n",
    "        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        # model6+=[norm_layer(512),]\n",
    "        model6+=[nn.ReLU(True),]\n",
    "        # model6+=[nn.ReflectionPad2d(2),]\n",
    "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        # model6+=[norm_layer(512),]\n",
    "        model6+=[nn.ReLU(True),]\n",
    "        # model6+=[nn.ReflectionPad2d(2),]\n",
    "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        model6+=[nn.ReLU(True),]\n",
    "        model6+=[norm_layer(512),]\n",
    "\n",
    "        # Conv7\n",
    "        # model47+=[nn.ReflectionPad2d(1),]\n",
    "        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model7+=[norm_layer(512),]\n",
    "        model7+=[nn.ReLU(True),]\n",
    "        # model7+=[nn.ReflectionPad2d(1),]\n",
    "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model7+=[norm_layer(512),]\n",
    "        model7+=[nn.ReLU(True),]\n",
    "        # model7+=[nn.ReflectionPad2d(1),]\n",
    "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model7+=[nn.ReLU(True),]\n",
    "        model7+=[norm_layer(512),]\n",
    "\n",
    "        # Conv8\n",
    "        model8up=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=use_bias)]\n",
    "\n",
    "        # model3short8=[nn.ReflectionPad2d(1),]\n",
    "        model3short8=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "\n",
    "        # model47+=[norm_layer(256),]\n",
    "        model8=[nn.ReLU(True),]\n",
    "        # model8+=[nn.ReflectionPad2d(1),]\n",
    "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model8+=[norm_layer(256),]\n",
    "        model8+=[nn.ReLU(True),]\n",
    "        # model8+=[nn.ReflectionPad2d(1),]\n",
    "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model8+=[nn.ReLU(True),]\n",
    "        model8+=[norm_layer(256),]\n",
    "\n",
    "        # Conv9\n",
    "        model9up=[nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),]\n",
    "\n",
    "        # model2short9=[nn.ReflectionPad2d(1),]\n",
    "        model2short9=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # add the two feature maps above        \n",
    "\n",
    "        # model9=[norm_layer(128),]\n",
    "        model9=[nn.ReLU(True),]\n",
    "        # model9+=[nn.ReflectionPad2d(1),]\n",
    "        model9+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model9+=[nn.ReLU(True),]\n",
    "        model9+=[norm_layer(128),]\n",
    "\n",
    "        # Conv10\n",
    "        model10up=[nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),]\n",
    "\n",
    "        # model1short10=[nn.ReflectionPad2d(1),]\n",
    "        model1short10=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # add the two feature maps above\n",
    "\n",
    "        # model10=[norm_layer(128),]\n",
    "        model10=[nn.ReLU(True),]\n",
    "        # model10+=[nn.ReflectionPad2d(1),]\n",
    "        model10+=[nn.Conv2d(128, 128, kernel_size=3, dilation=1, stride=1, padding=1, bias=use_bias),]\n",
    "        model10+=[nn.LeakyReLU(negative_slope=.2),]\n",
    "\n",
    "        # classification output\n",
    "        model_class=[nn.Conv2d(256, 529, kernel_size=1, padding=0, dilation=1, stride=1, bias=use_bias),]\n",
    "\n",
    "        # regression output\n",
    "        model_out=[nn.Conv2d(128, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=use_bias),]\n",
    "        if(use_tanh):\n",
    "            model_out+=[nn.Tanh()]\n",
    "\n",
    "        self.model1 = nn.Sequential(*model1)\n",
    "        self.model2 = nn.Sequential(*model2)\n",
    "        self.model3 = nn.Sequential(*model3)\n",
    "        self.model4 = nn.Sequential(*model4)\n",
    "        self.att4 = CBAM(512)\n",
    "        self.model5 = nn.Sequential(*model5)\n",
    "        #self.att5 = CBAM(512)\n",
    "        self.model6 = nn.Sequential(*model6)\n",
    "        #self.att6 = CBAM(512)\n",
    "        self.model7 = nn.Sequential(*model7)\n",
    "        self.model8up = nn.Sequential(*model8up)\n",
    "        self.model8 = nn.Sequential(*model8)\n",
    "        self.model9up = nn.Sequential(*model9up)\n",
    "        self.model9 = nn.Sequential(*model9)\n",
    "        self.model10up = nn.Sequential(*model10up)\n",
    "        self.model10 = nn.Sequential(*model10)\n",
    "        self.model3short8 = nn.Sequential(*model3short8)\n",
    "        self.model2short9 = nn.Sequential(*model2short9)\n",
    "        self.model1short10 = nn.Sequential(*model1short10)\n",
    "\n",
    "        self.model_class = nn.Sequential(*model_class)\n",
    "        self.model_out = nn.Sequential(*model_out)\n",
    "        self.att_out = CBAM(128)  # у conv10_2 выход 128 каналов\n",
    "\n",
    "        self.upsample4 = nn.Sequential(*[nn.Upsample(scale_factor=4, mode='nearest'),])\n",
    "        self.softmax = nn.Sequential(*[nn.Softmax(dim=1),])\n",
    "\n",
    "    def forward(self, input_A, input_B, mask_B):\n",
    "        conv1_2 = self.model1(torch.cat((input_A,input_B,mask_B),dim=1))\n",
    "        conv2_2 = self.model2(conv1_2[:,:,::2,::2])\n",
    "        conv3_3 = self.model3(conv2_2[:,:,::2,::2])\n",
    "        conv4_3 = self.model4(conv3_3[:,:,::2,::2])\n",
    "        conv4_3 = self.att4(conv4_3)\n",
    "        conv5_3 = self.model5(conv4_3)\n",
    "        #conv5_3 = self.att5(conv5_3)\n",
    "        conv6_3 = self.model6(conv5_3)\n",
    "        #conv6_3 = self.att6(conv6_3)\n",
    "        conv7_3 = self.model7(conv6_3)\n",
    "        conv8_up = self.model8up(conv7_3) + self.model3short8(conv3_3)\n",
    "        conv8_3 = self.model8(conv8_up)\n",
    "\n",
    "        if(self.classification):\n",
    "            out_class = self.model_class(conv8_3)\n",
    "            conv9_up = self.model9up(conv8_3.detach()) + self.model2short9(conv2_2.detach())\n",
    "            conv9_3 = self.model9(conv9_up)\n",
    "            conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2.detach())\n",
    "            conv10_2 = self.model10(conv10_up)\n",
    "            conv10_2 = self.att_out(conv10_2)  # <-- attention перед финальным выходом\n",
    "            out_reg = self.model_out(conv10_2)\n",
    "\n",
    "        else:\n",
    "            out_class = self.model_class(conv8_3.detach())\n",
    "\n",
    "            conv9_up = self.model9up(conv8_3) + self.model2short9(conv2_2)\n",
    "            conv9_3 = self.model9(conv9_up)\n",
    "            conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2)\n",
    "            conv10_2 = self.model10(conv10_up)\n",
    "            conv10_2 = self.att_out(conv10_2)  # <-- attention перед финальным выходом\n",
    "            out_reg = self.model_out(conv10_2)\n",
    "\n",
    "\n",
    "        return (out_class, out_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3159b74d-a8a6-4c65-86df-8354aa520085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#старый модель без улучшения\n",
    "\n",
    "class SIGGRAPHGenerator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, norm_layer=nn.BatchNorm2d, use_tanh=True, classification=True):\n",
    "        super(SIGGRAPHGenerator, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.classification = classification\n",
    "        use_bias = True\n",
    "\n",
    "        # Conv1\n",
    "        # model1=[nn.ReflectionPad2d(1),]\n",
    "        model1=[nn.Conv2d(input_nc, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model1+=[norm_layer(64),]\n",
    "        model1+=[nn.ReLU(True),]\n",
    "        # model1+=[nn.ReflectionPad2d(1),]\n",
    "        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model1+=[nn.ReLU(True),]\n",
    "        model1+=[norm_layer(64),]\n",
    "        # add a subsampling operation\n",
    "\n",
    "        # Conv2\n",
    "        # model2=[nn.ReflectionPad2d(1),]\n",
    "        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model2+=[norm_layer(128),]\n",
    "        model2+=[nn.ReLU(True),]\n",
    "        # model2+=[nn.ReflectionPad2d(1),]\n",
    "        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model2+=[nn.ReLU(True),]\n",
    "        model2+=[norm_layer(128),]\n",
    "        # add a subsampling layer operation\n",
    "\n",
    "        # Conv3\n",
    "        # model3=[nn.ReflectionPad2d(1),]\n",
    "        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model3+=[norm_layer(256),]\n",
    "        model3+=[nn.ReLU(True),]\n",
    "        # model3+=[nn.ReflectionPad2d(1),]\n",
    "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model3+=[norm_layer(256),]\n",
    "        model3+=[nn.ReLU(True),]\n",
    "        # model3+=[nn.ReflectionPad2d(1),]\n",
    "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model3+=[nn.ReLU(True),]\n",
    "        model3+=[norm_layer(256),]\n",
    "        # add a subsampling layer operation\n",
    "\n",
    "        # Conv4\n",
    "        # model47=[nn.ReflectionPad2d(1),]\n",
    "        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model4+=[norm_layer(512),]\n",
    "        model4+=[nn.ReLU(True),]\n",
    "        # model4+=[nn.ReflectionPad2d(1),]\n",
    "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model4+=[norm_layer(512),]\n",
    "        model4+=[nn.ReLU(True),]\n",
    "        # model4+=[nn.ReflectionPad2d(1),]\n",
    "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model4+=[nn.ReLU(True),]\n",
    "        model4+=[norm_layer(512),]\n",
    "\n",
    "        # Conv5\n",
    "        # model47+=[nn.ReflectionPad2d(2),]\n",
    "        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        # model5+=[norm_layer(512),]\n",
    "        model5+=[nn.ReLU(True),]\n",
    "        # model5+=[nn.ReflectionPad2d(2),]\n",
    "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        # model5+=[norm_layer(512),]\n",
    "        model5+=[nn.ReLU(True),]\n",
    "        # model5+=[nn.ReflectionPad2d(2),]\n",
    "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        model5+=[nn.ReLU(True),]\n",
    "        model5+=[norm_layer(512),]\n",
    "\n",
    "        # Conv6\n",
    "        # model6+=[nn.ReflectionPad2d(2),]\n",
    "        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        # model6+=[norm_layer(512),]\n",
    "        model6+=[nn.ReLU(True),]\n",
    "        # model6+=[nn.ReflectionPad2d(2),]\n",
    "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        # model6+=[norm_layer(512),]\n",
    "        model6+=[nn.ReLU(True),]\n",
    "        # model6+=[nn.ReflectionPad2d(2),]\n",
    "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        model6+=[nn.ReLU(True),]\n",
    "        model6+=[norm_layer(512),]\n",
    "\n",
    "        # Conv7\n",
    "        # model47+=[nn.ReflectionPad2d(1),]\n",
    "        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model7+=[norm_layer(512),]\n",
    "        model7+=[nn.ReLU(True),]\n",
    "        # model7+=[nn.ReflectionPad2d(1),]\n",
    "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model7+=[norm_layer(512),]\n",
    "        model7+=[nn.ReLU(True),]\n",
    "        # model7+=[nn.ReflectionPad2d(1),]\n",
    "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model7+=[nn.ReLU(True),]\n",
    "        model7+=[norm_layer(512),]\n",
    "\n",
    "        # Conv7\n",
    "        model8up=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=use_bias)]\n",
    "\n",
    "        # model3short8=[nn.ReflectionPad2d(1),]\n",
    "        model3short8=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "\n",
    "        # model47+=[norm_layer(256),]\n",
    "        model8=[nn.ReLU(True),]\n",
    "        # model8+=[nn.ReflectionPad2d(1),]\n",
    "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # model8+=[norm_layer(256),]\n",
    "        model8+=[nn.ReLU(True),]\n",
    "        # model8+=[nn.ReflectionPad2d(1),]\n",
    "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model8+=[nn.ReLU(True),]\n",
    "        model8+=[norm_layer(256),]\n",
    "\n",
    "        # Conv9\n",
    "        model9up=[nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),]\n",
    "\n",
    "        # model2short9=[nn.ReflectionPad2d(1),]\n",
    "        model2short9=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # add the two feature maps above        \n",
    "\n",
    "        # model9=[norm_layer(128),]\n",
    "        model9=[nn.ReLU(True),]\n",
    "        # model9+=[nn.ReflectionPad2d(1),]\n",
    "        model9+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model9+=[nn.ReLU(True),]\n",
    "        model9+=[norm_layer(128),]\n",
    "\n",
    "        # Conv10\n",
    "        model10up=[nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),]\n",
    "\n",
    "        # model1short10=[nn.ReflectionPad2d(1),]\n",
    "        model1short10=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        # add the two feature maps above\n",
    "\n",
    "        # model10=[norm_layer(128),]\n",
    "        model10=[nn.ReLU(True),]\n",
    "        # model10+=[nn.ReflectionPad2d(1),]\n",
    "        model10+=[nn.Conv2d(128, 128, kernel_size=3, dilation=1, stride=1, padding=1, bias=use_bias),]\n",
    "        model10+=[nn.LeakyReLU(negative_slope=.2),]\n",
    "\n",
    "        # classification output\n",
    "        model_class=[nn.Conv2d(256, 529, kernel_size=1, padding=0, dilation=1, stride=1, bias=use_bias),]\n",
    "\n",
    "        # regression output\n",
    "        model_out=[nn.Conv2d(128, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=use_bias),]\n",
    "        if(use_tanh):\n",
    "            model_out+=[nn.Tanh()]\n",
    "\n",
    "        self.model1 = nn.Sequential(*model1)\n",
    "        self.model2 = nn.Sequential(*model2)\n",
    "        self.model3 = nn.Sequential(*model3)\n",
    "        self.model4 = nn.Sequential(*model4)\n",
    "        self.model5 = nn.Sequential(*model5)\n",
    "        self.model6 = nn.Sequential(*model6)\n",
    "        self.model7 = nn.Sequential(*model7)\n",
    "        self.model8up = nn.Sequential(*model8up)\n",
    "        self.model8 = nn.Sequential(*model8)\n",
    "        self.model9up = nn.Sequential(*model9up)\n",
    "        self.model9 = nn.Sequential(*model9)\n",
    "        self.model10up = nn.Sequential(*model10up)\n",
    "        self.model10 = nn.Sequential(*model10)\n",
    "        self.model3short8 = nn.Sequential(*model3short8)\n",
    "        self.model2short9 = nn.Sequential(*model2short9)\n",
    "        self.model1short10 = nn.Sequential(*model1short10)\n",
    "\n",
    "        self.model_class = nn.Sequential(*model_class)\n",
    "        self.model_out = nn.Sequential(*model_out)\n",
    "\n",
    "        self.upsample4 = nn.Sequential(*[nn.Upsample(scale_factor=4, mode='nearest'),])\n",
    "        self.softmax = nn.Sequential(*[nn.Softmax(dim=1),])\n",
    "\n",
    "    def forward(self, input_A, input_B, mask_B):\n",
    "        conv1_2 = self.model1(torch.cat((input_A,input_B,mask_B),dim=1))\n",
    "        conv2_2 = self.model2(conv1_2[:,:,::2,::2])\n",
    "        conv3_3 = self.model3(conv2_2[:,:,::2,::2])\n",
    "        conv4_3 = self.model4(conv3_3[:,:,::2,::2])\n",
    "        conv5_3 = self.model5(conv4_3)\n",
    "        conv6_3 = self.model6(conv5_3)\n",
    "        conv7_3 = self.model7(conv6_3)\n",
    "        conv8_up = self.model8up(conv7_3) + self.model3short8(conv3_3)\n",
    "        conv8_3 = self.model8(conv8_up)\n",
    "\n",
    "        if(self.classification):\n",
    "            out_class = self.model_class(conv8_3)\n",
    "            conv9_up = self.model9up(conv8_3.detach()) + self.model2short9(conv2_2.detach())\n",
    "            conv9_3 = self.model9(conv9_up)\n",
    "            conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2.detach())\n",
    "            conv10_2 = self.model10(conv10_up)\n",
    "            out_reg = self.model_out(conv10_2)\n",
    "        else:\n",
    "            out_class = self.model_class(conv8_3.detach())\n",
    "\n",
    "            conv9_up = self.model9up(conv8_3) + self.model2short9(conv2_2)\n",
    "            conv9_3 = self.model9(conv9_up)\n",
    "            conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2)\n",
    "            conv10_2 = self.model10(conv10_up)\n",
    "            out_reg = self.model_out(conv10_2)\n",
    "\n",
    "        return (out_class, out_reg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
